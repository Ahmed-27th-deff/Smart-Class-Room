# -*- coding: utf-8 -*-
"""Copy of final_p_nti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L6wGooFDxtlXjxXxYKsKi52hdU68-xVj

# **INSTALL LIBRARIES**
"""

# update huggingface
!pip install huggingface_hub==0.34.3 --upgrade

!pip install insightface torch torchvision yolov5

!pip install onnxruntime

!pip install deep_sort_realtime

"""# **IMPORT LIBRARIES**"""

import cv2
import torch
import pickle
import numpy as np
from collections import defaultdict
from insightface.app import FaceAnalysis
from scipy.spatial.distance import cosine
from deep_sort_realtime.deepsort_tracker import DeepSort
import torchvision.transforms.functional as F
import csv
from torchvision.models.detection import keypointrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from IPython.display import clear_output
import time
from datetime import datetime
import os
import torchvision
from google.colab.patches import cv2_imshow

face_app = FaceAnalysis(name='buffalo_l')
face_app.prepare(ctx_id=0, det_size=(640, 640))  # Use CPU (ctx_id=0)

"""
# **LOAD STUDENT IMAGES**
"""

students_dir = "students"  # Folder with student images
known_embeddings = []
known_names = []

for filename in os.listdir(students_dir):
    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
        path = os.path.join(students_dir, filename)
        img = cv2.imread(path)
        if img is None:
            print(f" Failed to load: {filename}")
            continue

        faces = face_app.get(img)
        if not faces:
            print(f" No face found in {filename}")
            continue

        embedding = faces[0].embedding
        name = os.path.splitext(filename)[0]  # Use filename as name

        known_embeddings.append(embedding)
        known_names.append(name)
        print(f" Added: {name}")

"""
# **SAVE ENCODINGS TO FILE**
"""

output_path = "student_embeddings.pkl"
with open(output_path, "wb") as f:
    pickle.dump({
        "embeddings": np.array(known_embeddings),
        "names": known_names
    }, f)

print("\n Embeddings saved to:", output_path)
print(" Total students enrolled:", len(known_names))

"""# **CONFIG**

"""

VIDEO_PATH = "/content/classroom_clip.mp4"
PKL_PATH = "/content/student_embeddings.pkl"
OUTPUT_VIDEO = "output.mp4"
CSV_OUTPUT = "hand_raise_report.csv"
THRESHOLD = 0.9  # Face recognition threshold

"""# **LOAD KNOWN FACES**

"""

print("[INFO] Loading known faces...")
with open(PKL_PATH, 'rb') as f:
    data = pickle.load(f)
known_embeddings = data['embeddings']
known_names = data['names']
print(f"[INFO] Loaded {len(known_names)} known faces.")

"""# **INIT MODELS**

"""

print("[INFO] Initializing models...")
# Face recognition
face_app = FaceAnalysis(name='buffalo_l')
face_app.prepare(ctx_id=0, det_size=(640, 640))

# YOLOv5
model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s')
model_yolo.classes = [0]  # Only detect persons

# Deep SORT
tracker = DeepSort(max_age=30, n_init=3)

# Keypoint pose estimation model (torch)
pose_model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)
pose_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pose_model.to(device)

"""# **HAND RAISE COUNTER**

"""

# CONSTANTS
VERTICAL_THRESHOLD_RATIO = 0.35
HAND_SHOULDER_DISTANCE_RATIO = 0.8

hand_raise_counts = defaultdict(int)
raise_state = defaultdict(int)  # Track 1/0 state per person (track_id)
student_names = defaultdict(lambda: "Unknown")  # Map track_id to student name

"""# **VIDEO PROCESSING**

"""

cap = cv2.VideoCapture(VIDEO_PATH)
out = None
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = cap.get(cv2.CAP_PROP_FPS)

print("[INFO] Processing video...")
frame_index = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_index += 1
    print(f"\n[FRAME {frame_index}]")
    poseimg = frame.copy()
    # YOLO detection
    results = model_yolo(frame)
    detections = results.xyxy[0].cpu().numpy()
    print(f"[YOLO] Detected {len(detections)} people.")

    # Deep SORT input format
    deep_sort_inputs = []
    for *xyxy, conf, cls in detections:
        x1, y1, x2, y2 = map(int, xyxy)
        conf = float(conf)
        deep_sort_inputs.append(([x1, y1, x2 - x1, y2 - y1], conf, frame[y1:y2, x1:x2]))

    tracks = tracker.update_tracks(deep_sort_inputs, frame=frame)
    print(f"[Deep SORT] Active tracks: {len(tracks)}")

    # Step 1: Detect all faces in the full frame
    all_faces = face_app.get(frame)

    # Step 2: Match each face to the closest track and assign names
    used_faces = set()
    for track in tracks:
        if not track.is_confirmed():
            continue

        track_id = track.track_id
        l, t, r, b = track.to_ltrb()
        x1, y1, x2, y2 = map(int, [l, t, r, b])

        # Get track center
        track_center = np.array([(x1 + x2) // 2, (y1 + y2) // 2])

        closest_face = None
        min_dist = float('inf')

        for i, face in enumerate(all_faces):
            if i in used_faces:
                continue
            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
            face_center = np.array([(fx1 + fx2) // 2, (fy1 + fy2) // 2])
            dist = np.linalg.norm(track_center - face_center)
            if dist < min_dist:
                min_dist = dist
                closest_face = (i, face)

        if closest_face is not None:
            i, face = closest_face
            used_faces.add(i)
            name = "Unknown"
            best_score = 1.0
            emb = face.embedding
            for ref_emb, ref_name in zip(known_embeddings, known_names):
                score = cosine(emb, ref_emb)
                if score < best_score and score < THRESHOLD:
                    best_score = score
                    name = ref_name
            student_names[track_id] = name  # Update name for this track_id

            # Draw face box
            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
            cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
            cv2.putText(frame, f"{name}", (fx1, fy1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            cv2.putText(frame, f'ID: {track_id}', (fx1, fy1 - 35),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    # Convert frame to tensor for PyTorch
    img = F.to_tensor(frame).to(device)

    # Run pose estimation
    with torch.no_grad():
        predictions = pose_model([img])

    # Extract keypoints and confidence scores
    keypoints = predictions[0]['keypoints'].cpu().numpy()
    scores = predictions[0]['scores'].cpu().numpy()

    # Filter detections with high confidence (score > 0.9)
    high_conf_idx = np.where(scores > 0.9)[0]
    keypoints = keypoints[high_conf_idx]

    # Define keypoint indices (COCO format)
    NOSE = 0
    LEFT_WRIST = 9
    RIGHT_WRIST = 10
    LEFT_SHOULDER = 5
    RIGHT_SHOULDER = 6
    LEFT_HIP = 11
    RIGHT_HIP = 12

    # Store per-person raised hand status
    raised_flags = []
    label_info = []
    track_keypoints = {}  # Map track_id to keypoints

    # Match keypoints to tracks based on bounding box overlap
    for person in keypoints:
        # Compute person bounding box from keypoints
        valid_kp = person[person[:, 2] > 0.2]
        if len(valid_kp) == 0:
            continue
        x_min, y_min = valid_kp[:, 0].min(), valid_kp[:, 1].min()
        x_max, y_max = valid_kp[:, 0].max(), valid_kp[:, 1].max()

        # Find closest track
        best_iou = 0
        best_track_id = None
        for track in tracks:
            if not track.is_confirmed():
                continue
            l, t, r, b = track.to_ltrb()
            x1, y1, x2, y2 = map(int, [l, t, r, b])
            # Compute IoU
            xx1 = max(x_min, x1)
            yy1 = max(y_min, y1)
            xx2 = min(x_max, x2)
            yy2 = min(y_max, y2)
            w = max(0, xx2 - xx1)
            h = max(0, yy2 - yy1)
            intersection = w * h
            union = (x_max - x_min) * (y_max - y_min) + (x2 - x1) * (y2 - y1) - intersection
            iou = intersection / union if union > 0 else 0
            if iou > best_iou:
                best_iou = iou
                best_track_id = track.track_id

        if best_track_id is not None and best_iou > 0.3:
            track_keypoints[best_track_id] = person

    # Process each track for hand raise detection
    for track in tracks:
        if not track.is_confirmed() or track.track_id not in track_keypoints:
            continue
        person = track_keypoints[track.track_id]
        track_id = track.track_id

        # Get relevant keypoints
        nose = person[NOSE]
        left_wrist = person[LEFT_WRIST]
        right_wrist = person[RIGHT_WRIST]
        left_shoulder = person[LEFT_SHOULDER]
        right_shoulder = person[RIGHT_SHOULDER]
        left_hip = person[LEFT_HIP]
        right_hip = person[RIGHT_HIP]

        # Calculate torso height
        torso_height = 0
        valid_torso_points = 0

        if left_shoulder[2] > 0.2 and left_hip[2] > 0.2:
            torso_height += abs(left_shoulder[1] - left_hip[1])
            valid_torso_points += 1

        if right_shoulder[2] > 0.2 and right_hip[2] > 0.2:
            torso_height += abs(right_shoulder[1] - right_hip[1])
            valid_torso_points += 1

        if valid_torso_points > 0:
            torso_height /= valid_torso_points
        else:
            if left_shoulder[2] > 0.2 and right_shoulder[2] > 0.2:
                shoulder_width = abs(left_shoulder[0] - right_shoulder[0])
                torso_height = shoulder_width * 1.2
            else:
                torso_height = 50

        # Calculate vertical raise threshold
        vertical_threshold = torso_height * VERTICAL_THRESHOLD_RATIO

        # Check if hands are raised
        left_raised = False
        right_raised = False

        # Left hand check
        if (left_wrist[2] > 0.2 and left_shoulder[2] > 0.2 and
            right_shoulder[2] > 0.2):
            vertical_check = left_shoulder[1] - left_wrist[1] > vertical_threshold
            dist_to_shoulder = np.linalg.norm(left_wrist[:2] - left_shoulder[:2])
            max_natural_dist = torso_height * HAND_SHOULDER_DISTANCE_RATIO
            distance_check = dist_to_shoulder > max_natural_dist
            left_raised = vertical_check and distance_check

        # Right hand check
        if (right_wrist[2] > 0.2 and right_shoulder[2] > 0.2 and
            left_shoulder[2] > 0.2):
            vertical_check = right_shoulder[1] - right_wrist[1] > vertical_threshold
            dist_to_shoulder = np.linalg.norm(right_wrist[:2] - right_shoulder[:2])
            max_natural_dist = torso_height * HAND_SHOULDER_DISTANCE_RATIO
            distance_check = dist_to_shoulder > max_natural_dist
            right_raised = vertical_check and distance_check

        # Set per-person flag
        raised = left_raised or right_raised
        raised_flags.append(raised)

        # Update hand raise state and count
        prev_state = raise_state[track_id]
        if raised and prev_state == 0:
            # Transition from not raised to raised
            raise_state[track_id] = 1
            hand_raise_counts[track_id] += 1
            print(f"[Hand Raise] {student_names[track_id]} (ID: {track_id}) raised hand, count: {hand_raise_counts[track_id]}")
        elif not raised and prev_state == 1:
            # Transition from raised to not raised
            raise_state[track_id] = 0

        # Determine position for label
        label_pos = None
        if nose[2] > 0.2:
            label_pos = (nose[0], nose[1] - 20)
        elif left_shoulder[2] > 0.2 and right_shoulder[2] > 0.2:
            mid_x = (left_shoulder[0] + right_shoulder[0]) / 2
            mid_y = (left_shoulder[1] + right_shoulder[1]) / 2
            label_pos = (mid_x, mid_y - 20)
        elif left_shoulder[2] > 0.2:
            label_pos = (left_shoulder[0], left_shoulder[1] - 20)
        elif right_shoulder[2] > 0.2:
            label_pos = (right_shoulder[0], right_shoulder[1] - 20)

        if label_pos is not None:
            label_info.append((label_pos, raised, track_id))

    # Count people with raised hands
    people_with_raised_hands = sum(raised_flags)


    # Draw per-person status labels
    for pos, raised, track_id in label_info:
        label = f"{student_names[track_id]}: {'Raised' if raised else 'Not Raised'} ({raise_state[track_id]})"
        color = (0, 255, 0) if raised else (0, 0, 255)
        cv2.putText(frame, label, (int(pos[0]), int(pos[1])),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Display total count
    cv2.putText(frame, f'Raised Hands: {people_with_raised_hands}/{len(raised_flags)}', (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    if out is None:
        h, w = frame.shape[:2]
        out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))

    out.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()

# -----------------------------
# EXPORT CSV REPORT
# -----------------------------
print("\n Final Hand Raise Report:")
# Get current date
current_date = datetime.now().strftime("%Y-%m-%d")
# Total students from known faces (assuming 25 students as per your input)
total_students = len(known_names)
# Detected students (present)
detected_names = set(student_names.values()) - {"Unknown"}
total_attendance = len(detected_names)
# Absent students (in known_names but not detected)
total_absences = total_students - total_attendance

with open(CSV_OUTPUT, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    # Write summary headers and data
    writer.writerow(["Date", "Total Students", "Total Attendance", "Total Absences"])
    writer.writerow([current_date, total_students, total_attendance, total_absences])
    writer.writerow([])  # Empty row for separation
    # Write student details header
    writer.writerow(["Student", "Track ID", "Hand Raises"])
    # Write data for detected students
    for track_id, count in hand_raise_counts.items():
        student = student_names[track_id]
        print(f"{student} (ID: {track_id}): {count}")
        writer.writerow([student, track_id, count])
    # Write absent students (no hand raises, not detected)
    for name in known_names:
        if name not in detected_names:
            print(f"{name} (ID: None): 0 (Absent)")
            writer.writerow([name, "None","Absent" ])

print(f"\n Report saved as '{CSV_OUTPUT}'")
print(f" Video saved as '{OUTPUT_VIDEO}'")